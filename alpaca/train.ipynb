{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from utils.prompter import Prompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    # model/data params\n",
    "    base_model: str = \"\",  # the only required argument\n",
    "    data_path: str = \"yahma/alpaca-cleaned\",\n",
    "    output_dir: str = \"./lora-alpaca\",\n",
    "    \n",
    "    # training hyperparams\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    \n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\"q_proj\", \"v_proj\"],\n",
    "    \n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "    resume_from_checkpoint: str = None,  # either training checkpoint or final adapter\n",
    "    prompt_template_name: str = \"alpaca\",  # The prompt template to use, will default to alpaca.\n",
    "):\n",
    "    print(\n",
    "        f\"Training Alpaca-LoRA model with params:\\n\"\n",
    "        f\"base_model: {base_model}\\n\"\n",
    "        f\"data_path: {data_path}\\n\"\n",
    "        f\"output_dir: {output_dir}\\n\"\n",
    "        f\"batch_size: {batch_size}\\n\"\n",
    "        f\"micro_batch_size: {micro_batch_size}\\n\"\n",
    "        f\"num_epochs: {num_epochs}\\n\"\n",
    "        f\"learning_rate: {learning_rate}\\n\"\n",
    "        f\"cutoff_len: {cutoff_len}\\n\"\n",
    "        f\"lora_r: {lora_r}\\n\"\n",
    "        f\"lora_alpha: {lora_alpha}\\n\"\n",
    "        f\"lora_dropout: {lora_dropout}\\n\"\n",
    "        f\"lora_target_modules: {lora_target_modules}\\n\"\n",
    "        f\"train_on_inputs: {train_on_inputs}\\n\"\n",
    "        f\"add_eos_token: {add_eos_token}\\n\"\n",
    "        f\"group_by_length: {group_by_length}\\n\"\n",
    "        f\"resume_from_checkpoint: {resume_from_checkpoint or False}\\n\"\n",
    "        f\"prompt template: {prompt_template_name}\\n\"\n",
    "    )\n",
    "    \n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint_name = os.path.join(resume_from_checkpoint, \"pytorch_model.bin\")\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(resume_from_checkpoint, \"adapter_model.bin\")\n",
    "            resume_from_checkpoint = (False)\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    tokenizer.pad_token_id = (0)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def tokenize(data_point):\n",
    "        prompt = data_point[\"text\"]\n",
    "        \n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def split_into_smaller_windows(samples):\n",
    "        texts = samples[\"text\"]\n",
    "        splits = []\n",
    "        for text in texts:\n",
    "            text_splits = [text[i:i+cutoff_len] for i in range(0, len(text), cutoff_len)]\n",
    "            splits.extend(text_splits)\n",
    "        samples[\"text\"] = splits\n",
    "\n",
    "    data = load_dataset(data_path)\n",
    "\n",
    "    train_data = data[\"train\"].shuffle().map(tokenize, num_proc=16).map(split_into_smaller_windows, batched=True, num_proc=16)\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=None,\n",
    "            save_steps=1,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=100,\n",
    "            load_best_model_at_end=False,\n",
    "            ddp_find_unused_parameters=None,\n",
    "            group_by_length=group_by_length,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Alpaca-LoRA model with params:\n",
      "base_model: decapoda-research/llama-7b-hf\n",
      "data_path: klima7/polish-prose\n",
      "output_dir: models/alpaca-prose-0\n",
      "batch_size: 128\n",
      "micro_batch_size: 8\n",
      "num_epochs: 100\n",
      "learning_rate: 0.0003\n",
      "cutoff_len: 512\n",
      "lora_r: 16\n",
      "lora_alpha: 16\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "train_on_inputs: True\n",
      "add_eos_token: False\n",
      "group_by_length: True\n",
      "resume_from_checkpoint: models/polpaca\n",
      "prompt template: alpaca\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:30<00:00,  1.07it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Downloading readme: 100%|██████████| 446/446 [00:00<00:00, 3.40MB/s]\n",
      "Downloading data: 100%|██████████| 154M/154M [00:41<00:00, 3.75MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:41<00:00, 41.10s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1370.24it/s]\n",
      "Generating train split: 100%|██████████| 1956/1956 [00:01<00:00, 1479.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting from models/polpaca/adapter_model.bin\n",
      "trainable params: 16777216 || all params: 6755192832 || trainable%: 0.24836028248556738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 1956/1956 [02:31<00:00, 12.93 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 1956/1956 [00:00<00:00, 3034.75 examples/s]\n",
      "  0%|          | 0/1500 [00:00<?, ?it/s]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 1/1500 [04:42<117:36:45, 282.46s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 2/1500 [08:06<98:19:24, 236.29s/it] /home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 3/1500 [11:22<90:41:10, 218.08s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 4/1500 [14:50<88:51:55, 213.85s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 5/1500 [18:55<93:31:24, 225.21s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 6/1500 [22:07<88:44:17, 213.83s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  0%|          | 7/1500 [25:29<87:09:54, 210.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 8/1500 [28:53<86:11:07, 207.95s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 9/1500 [32:11<84:54:15, 205.00s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 10/1500 [35:34<84:35:26, 204.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7091, 'learning_rate': 0.00017999999999999998, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 11/1500 [38:58<84:26:07, 204.14s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 12/1500 [42:16<83:41:31, 202.48s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 13/1500 [45:40<83:44:26, 202.73s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 14/1500 [49:03<83:44:01, 202.85s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 15/1500 [52:19<82:47:29, 200.71s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 16/1500 [55:36<82:20:09, 199.74s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 17/1500 [58:59<82:43:25, 200.81s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|          | 18/1500 [1:02:23<82:57:42, 201.53s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 19/1500 [1:05:42<82:36:18, 200.80s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 20/1500 [1:09:05<82:53:15, 201.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3794, 'learning_rate': 0.0002987919463087248, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 21/1500 [1:12:29<83:04:54, 202.23s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  1%|▏         | 22/1500 [1:15:51<82:57:58, 202.08s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 23/1500 [1:19:14<83:02:36, 202.41s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 24/1500 [1:22:37<83:04:07, 202.61s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 25/1500 [1:25:51<81:57:12, 200.02s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 26/1500 [1:29:14<82:13:54, 200.84s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 27/1500 [1:32:37<82:26:31, 201.49s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 28/1500 [1:35:52<81:37:37, 199.63s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 29/1500 [1:39:15<81:56:54, 200.55s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 30/1500 [1:42:37<82:09:56, 201.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2863, 'learning_rate': 0.00029677852348993286, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 31/1500 [1:45:44<80:16:34, 196.73s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 32/1500 [1:49:06<80:58:17, 198.57s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 33/1500 [1:52:30<81:30:13, 200.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 34/1500 [1:55:41<80:25:03, 197.48s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 35/1500 [1:59:04<80:57:47, 198.95s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 36/1500 [2:02:26<81:21:13, 200.05s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  2%|▏         | 37/1500 [2:05:40<80:32:21, 198.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 38/1500 [2:09:03<81:03:18, 199.59s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 39/1500 [2:12:26<81:24:40, 200.60s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 40/1500 [2:15:46<81:15:35, 200.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.241, 'learning_rate': 0.0002947651006711409, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 41/1500 [2:19:09<81:28:54, 201.05s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 42/1500 [2:22:31<81:38:04, 201.57s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 43/1500 [2:25:45<80:40:09, 199.32s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 44/1500 [2:29:08<81:01:59, 200.36s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 45/1500 [2:32:31<81:14:55, 201.03s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 46/1500 [2:35:42<80:01:15, 198.13s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 47/1500 [2:39:05<80:29:50, 199.44s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 48/1500 [2:42:27<80:50:03, 200.42s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 49/1500 [2:45:46<80:30:42, 199.75s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 50/1500 [2:49:08<80:46:57, 200.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1901, 'learning_rate': 0.00029275167785234896, 'epoch': 3.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 51/1500 [2:52:31<80:58:23, 201.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  3%|▎         | 52/1500 [2:55:47<80:21:11, 199.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▎         | 53/1500 [2:59:10<80:39:25, 200.67s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▎         | 54/1500 [3:02:33<80:51:30, 201.31s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▎         | 55/1500 [3:05:48<80:08:33, 199.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▎         | 56/1500 [3:09:11<80:25:41, 200.51s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 57/1500 [3:12:34<80:37:01, 201.12s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 58/1500 [3:15:49<79:54:45, 199.50s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 59/1500 [3:19:12<80:16:56, 200.57s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 60/1500 [3:22:35<80:26:52, 201.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1611, 'learning_rate': 0.000290738255033557, 'epoch': 3.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 61/1500 [3:25:49<79:31:22, 198.95s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 62/1500 [3:29:05<79:10:05, 198.20s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 63/1500 [3:32:27<79:36:50, 199.45s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 64/1500 [3:35:50<79:57:35, 200.46s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 65/1500 [3:39:06<79:21:33, 199.09s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 66/1500 [3:42:29<79:42:44, 200.11s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  4%|▍         | 67/1500 [3:45:51<79:57:32, 200.87s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 68/1500 [3:49:06<79:08:06, 198.94s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 69/1500 [3:52:28<79:30:13, 200.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 70/1500 [3:55:51<79:47:54, 200.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0938, 'learning_rate': 0.00028872483221476506, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 71/1500 [3:59:11<79:38:33, 200.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 72/1500 [4:02:34<79:49:41, 201.25s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 73/1500 [4:05:56<79:55:56, 201.65s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▍         | 74/1500 [4:09:14<79:19:50, 200.27s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 75/1500 [4:12:36<79:33:19, 200.98s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 76/1500 [4:15:59<79:43:12, 201.54s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 77/1500 [4:19:04<77:44:32, 196.68s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 78/1500 [4:22:27<78:24:35, 198.51s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 79/1500 [4:25:50<78:50:55, 199.76s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 80/1500 [4:29:03<78:02:18, 197.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0948, 'learning_rate': 0.0002867114093959731, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 81/1500 [4:32:26<78:33:28, 199.30s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  5%|▌         | 82/1500 [4:35:48<78:53:32, 200.29s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 83/1500 [4:39:04<78:16:45, 198.87s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 84/1500 [4:42:27<78:39:17, 199.97s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 85/1500 [4:45:49<78:56:01, 200.82s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 86/1500 [4:49:04<78:11:32, 199.07s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 87/1500 [4:52:27<78:31:45, 200.07s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 88/1500 [4:55:49<78:44:50, 200.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 89/1500 [4:59:09<78:36:00, 200.54s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 90/1500 [5:02:32<78:45:52, 201.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0306, 'learning_rate': 0.0002846979865771812, 'epoch': 5.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 91/1500 [5:05:54<78:54:00, 201.59s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 92/1500 [5:09:06<77:41:17, 198.63s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▌         | 93/1500 [5:12:29<78:05:05, 199.79s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▋         | 94/1500 [5:15:51<78:21:12, 200.62s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▋         | 95/1500 [5:19:09<77:58:07, 199.78s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▋         | 96/1500 [5:22:32<78:14:44, 200.63s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  6%|▋         | 97/1500 [5:25:55<78:28:40, 201.37s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 98/1500 [5:29:12<77:58:45, 200.23s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 99/1500 [5:32:35<78:11:36, 200.93s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 100/1500 [5:35:57<78:18:47, 201.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9838, 'learning_rate': 0.00028268456375838926, 'epoch': 6.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 101/1500 [5:39:10<77:13:55, 198.74s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 102/1500 [5:42:32<77:35:20, 199.80s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 103/1500 [5:45:54<77:50:16, 200.58s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 104/1500 [5:49:13<77:32:13, 199.95s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 105/1500 [5:52:35<77:45:43, 200.68s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 106/1500 [5:55:58<77:56:51, 201.30s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 107/1500 [5:59:11<76:54:42, 198.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 108/1500 [6:02:27<76:32:43, 197.96s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 109/1500 [6:05:50<77:01:27, 199.34s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 110/1500 [6:09:12<77:20:22, 200.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9578, 'learning_rate': 0.0002806711409395973, 'epoch': 7.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 111/1500 [6:12:30<77:02:45, 199.69s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  7%|▋         | 112/1500 [6:15:53<77:21:23, 200.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 113/1500 [6:19:16<77:30:04, 201.16s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 114/1500 [6:22:33<77:01:53, 200.08s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 115/1500 [6:25:55<77:14:06, 200.76s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 116/1500 [6:29:18<77:21:03, 201.20s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 117/1500 [6:32:33<76:38:30, 199.50s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 118/1500 [6:35:56<76:57:03, 200.45s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 119/1500 [6:39:18<77:07:24, 201.05s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 120/1500 [6:42:33<76:20:13, 199.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9108, 'learning_rate': 0.0002786577181208053, 'epoch': 7.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 121/1500 [6:45:56<76:40:44, 200.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 122/1500 [6:49:18<76:51:02, 200.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 123/1500 [6:52:25<75:11:18, 196.57s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 124/1500 [6:55:47<75:48:23, 198.33s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 125/1500 [6:59:09<76:12:14, 199.52s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 126/1500 [7:02:25<75:45:56, 198.51s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  8%|▊         | 127/1500 [7:05:48<76:09:05, 199.67s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▊         | 128/1500 [7:09:11<76:28:29, 200.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▊         | 129/1500 [7:12:22<75:19:05, 197.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▊         | 130/1500 [7:15:44<75:48:14, 199.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8455, 'learning_rate': 0.0002766442953020134, 'epoch': 8.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▊         | 131/1500 [7:19:07<76:08:43, 200.24s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 132/1500 [7:22:22<75:28:01, 198.60s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 133/1500 [7:25:45<75:54:51, 199.92s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 134/1500 [7:29:07<76:08:32, 200.67s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 135/1500 [7:32:20<75:14:12, 198.43s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 136/1500 [7:35:43<75:38:04, 199.62s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 137/1500 [7:39:05<75:55:26, 200.53s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 138/1500 [7:42:20<75:09:53, 198.67s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 139/1500 [7:45:42<75:33:34, 199.86s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 140/1500 [7:49:05<75:48:43, 200.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8129, 'learning_rate': 0.00027463087248322146, 'epoch': 9.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 141/1500 [7:52:22<75:17:58, 199.47s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "  9%|▉         | 142/1500 [7:55:45<75:42:50, 200.71s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 143/1500 [7:59:42<79:45:58, 211.61s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 144/1500 [8:02:59<78:02:12, 207.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 145/1500 [8:06:25<77:50:28, 206.81s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 146/1500 [8:09:52<77:49:42, 206.93s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 147/1500 [8:13:08<76:32:43, 203.67s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 148/1500 [8:16:39<77:16:41, 205.77s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|▉         | 149/1500 [8:20:07<77:30:53, 206.55s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 150/1500 [8:23:30<77:01:20, 205.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7491, 'learning_rate': 0.0002726174496644295, 'epoch': 9.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 151/1500 [8:26:57<77:10:39, 205.96s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 152/1500 [8:30:25<77:17:20, 206.41s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 153/1500 [8:33:48<76:52:05, 205.44s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 154/1500 [8:37:09<76:18:57, 204.11s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 155/1500 [8:40:35<76:27:21, 204.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 156/1500 [8:44:01<76:31:46, 204.99s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 10%|█         | 157/1500 [8:47:19<75:41:47, 202.91s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 158/1500 [8:50:44<75:53:19, 203.58s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 159/1500 [8:54:09<75:58:00, 203.94s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 160/1500 [8:57:23<74:52:46, 201.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7213, 'learning_rate': 0.00027060402684563756, 'epoch': 10.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 161/1500 [9:00:50<75:27:32, 202.88s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 162/1500 [9:04:18<75:55:18, 204.27s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 163/1500 [9:07:45<76:08:33, 205.02s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 164/1500 [9:11:14<76:35:14, 206.37s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 165/1500 [9:14:43<76:51:29, 207.26s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 166/1500 [9:18:06<76:17:54, 205.90s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 167/1500 [9:21:36<76:42:56, 207.18s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█         | 168/1500 [9:25:07<77:02:08, 208.20s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█▏        | 169/1500 [9:28:24<75:44:31, 204.86s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█▏        | 170/1500 [9:32:00<76:53:54, 208.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6501, 'learning_rate': 0.0002685906040268456, 'epoch': 11.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█▏        | 171/1500 [9:35:33<77:22:23, 209.59s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 11%|█▏        | 172/1500 [9:38:57<76:43:14, 207.98s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 173/1500 [9:42:30<77:13:55, 209.52s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 174/1500 [9:46:00<77:14:39, 209.71s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 175/1500 [9:49:21<76:12:47, 207.07s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 176/1500 [9:52:50<76:20:07, 207.56s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 177/1500 [9:56:19<76:26:32, 208.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 178/1500 [9:59:40<75:39:00, 206.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 179/1500 [10:03:10<75:57:42, 207.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 180/1500 [10:06:38<76:05:05, 207.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6069, 'learning_rate': 0.00026657718120805365, 'epoch': 11.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 181/1500 [10:10:03<75:41:55, 206.61s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 182/1500 [10:13:33<75:59:57, 207.59s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 183/1500 [10:17:02<76:08:47, 208.15s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 184/1500 [10:20:17<74:37:25, 204.14s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 185/1500 [10:23:46<75:08:50, 205.73s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 186/1500 [10:27:15<75:27:40, 206.74s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 12%|█▏        | 187/1500 [10:30:38<74:54:42, 205.39s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 188/1500 [10:34:06<75:10:24, 206.27s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 189/1500 [10:37:34<75:15:36, 206.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 190/1500 [10:40:51<74:09:31, 203.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5585, 'learning_rate': 0.0002645637583892617, 'epoch': 12.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 191/1500 [10:44:19<74:33:36, 205.05s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 192/1500 [10:47:46<74:46:12, 205.79s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 193/1500 [10:51:08<74:15:22, 204.53s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 194/1500 [10:54:36<74:36:28, 205.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 195/1500 [10:58:04<74:51:03, 206.49s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 196/1500 [11:01:26<74:12:10, 204.85s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 197/1500 [11:04:57<74:53:02, 206.89s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 198/1500 [11:08:29<75:20:38, 208.32s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 199/1500 [11:11:57<75:14:41, 208.21s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 200/1500 [11:15:22<74:49:04, 207.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5109, 'learning_rate': 0.0002625503355704698, 'epoch': 13.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 201/1500 [11:18:52<75:03:57, 208.03s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 13%|█▎        | 202/1500 [11:22:24<75:27:57, 209.30s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▎        | 203/1500 [11:25:54<75:30:30, 209.58s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▎        | 204/1500 [11:29:28<75:58:16, 211.03s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▎        | 205/1500 [11:32:59<75:49:26, 210.78s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▎        | 206/1500 [11:36:21<74:51:40, 208.27s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 207/1500 [11:39:49<74:47:29, 208.24s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 208/1500 [11:43:15<74:24:41, 207.34s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 209/1500 [11:46:37<73:47:45, 205.78s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 210/1500 [11:50:06<74:07:08, 206.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4632, 'learning_rate': 0.00026053691275167786, 'epoch': 13.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 211/1500 [11:53:31<73:52:02, 206.30s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 212/1500 [11:56:52<73:12:00, 204.60s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 213/1500 [12:00:13<72:49:38, 203.71s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 214/1500 [12:03:30<72:03:42, 201.73s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 215/1500 [12:06:38<70:27:24, 197.39s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 216/1500 [12:10:01<71:00:40, 199.10s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 14%|█▍        | 217/1500 [12:13:22<71:09:54, 199.68s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 218/1500 [12:16:40<70:56:52, 199.23s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 219/1500 [12:20:04<71:24:10, 200.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 220/1500 [12:23:29<71:47:06, 201.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4274, 'learning_rate': 0.0002585234899328859, 'epoch': 14.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 221/1500 [12:26:45<71:06:40, 200.16s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 222/1500 [12:30:08<71:24:31, 201.15s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 223/1500 [12:33:31<71:28:48, 201.51s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▍        | 224/1500 [12:36:46<70:45:43, 199.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 225/1500 [12:40:09<71:05:32, 200.73s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 226/1500 [12:43:31<71:11:25, 201.17s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 227/1500 [12:46:48<70:40:59, 199.89s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 228/1500 [12:50:10<70:50:17, 200.49s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 229/1500 [12:53:35<71:13:31, 201.74s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 230/1500 [12:56:44<69:50:23, 197.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3655, 'learning_rate': 0.00025651006711409395, 'epoch': 15.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 231/1500 [13:00:05<70:07:02, 198.91s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 15%|█▌        | 232/1500 [13:03:27<70:22:40, 199.81s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 233/1500 [13:06:41<69:39:27, 197.92s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 234/1500 [13:10:04<70:14:03, 199.72s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 235/1500 [13:13:25<70:17:56, 200.06s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 236/1500 [13:16:43<70:00:29, 199.39s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 237/1500 [13:20:04<70:06:24, 199.83s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 238/1500 [13:23:24<70:05:37, 199.95s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 239/1500 [13:26:31<68:36:03, 195.85s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 240/1500 [13:29:50<68:54:47, 196.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3182, 'learning_rate': 0.000254496644295302, 'epoch': 15.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 241/1500 [13:33:09<69:06:28, 197.61s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 242/1500 [13:36:27<69:07:34, 197.82s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▌        | 243/1500 [13:39:49<69:25:06, 198.81s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▋        | 244/1500 [13:43:10<69:41:14, 199.74s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▋        | 245/1500 [13:46:19<68:28:25, 196.42s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▋        | 246/1500 [13:49:45<69:21:31, 199.12s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 16%|█▋        | 247/1500 [13:53:14<70:23:21, 202.24s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 248/1500 [13:56:33<69:57:02, 201.14s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 249/1500 [14:00:01<70:37:39, 203.25s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 250/1500 [14:03:30<71:12:52, 205.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.269, 'learning_rate': 0.00025248322147651005, 'epoch': 16.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 251/1500 [14:06:56<71:15:15, 205.38s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 252/1500 [14:10:27<71:44:19, 206.94s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 253/1500 [14:13:56<71:54:00, 207.57s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 254/1500 [14:17:16<71:06:54, 205.47s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 255/1500 [14:20:48<71:41:09, 207.29s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 256/1500 [14:24:17<71:48:59, 207.83s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 257/1500 [14:27:38<71:05:40, 205.91s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 258/1500 [14:31:07<71:20:35, 206.79s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 259/1500 [14:34:40<71:53:44, 208.56s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 260/1500 [14:38:07<71:41:58, 208.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2485, 'learning_rate': 0.0002504697986577181, 'epoch': 16.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 261/1500 [14:41:35<71:34:16, 207.96s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 17%|█▋        | 262/1500 [14:45:09<72:11:07, 209.91s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 263/1500 [14:48:44<72:39:34, 211.46s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 264/1500 [14:52:12<72:14:34, 210.42s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 265/1500 [14:55:43<72:15:37, 210.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 266/1500 [14:59:13<72:02:53, 210.19s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 267/1500 [15:02:29<70:36:39, 206.16s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 268/1500 [15:05:58<70:50:45, 207.02s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 269/1500 [15:09:28<71:04:02, 207.83s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 270/1500 [15:12:53<70:43:41, 207.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1886, 'learning_rate': 0.00024845637583892615, 'epoch': 17.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 271/1500 [15:16:23<70:59:19, 207.94s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 272/1500 [15:19:52<71:02:52, 208.28s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 273/1500 [15:23:16<70:34:00, 207.04s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 274/1500 [15:26:45<70:40:17, 207.52s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 275/1500 [15:30:16<70:55:04, 208.41s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 276/1500 [15:33:34<69:52:08, 205.50s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 18%|█▊        | 277/1500 [15:37:08<70:39:39, 208.00s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▊        | 278/1500 [15:40:42<71:09:20, 209.62s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▊        | 279/1500 [15:43:58<69:42:18, 205.52s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▊        | 280/1500 [15:47:30<70:18:40, 207.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1464, 'learning_rate': 0.0002464429530201342, 'epoch': 18.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▊        | 281/1500 [15:51:02<70:43:37, 208.87s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 282/1500 [15:54:28<70:22:27, 208.00s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 283/1500 [15:57:58<70:31:53, 208.64s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 284/1500 [16:01:27<70:32:59, 208.86s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 285/1500 [16:04:50<69:51:59, 207.01s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 286/1500 [16:08:20<70:05:40, 207.86s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 287/1500 [16:11:48<70:05:04, 208.00s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 288/1500 [16:15:14<69:47:43, 207.31s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 289/1500 [16:18:44<70:00:24, 208.11s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 290/1500 [16:22:12<70:00:46, 208.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1446, 'learning_rate': 0.00024442953020134225, 'epoch': 18.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 291/1500 [16:25:29<68:43:42, 204.65s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 19%|█▉        | 292/1500 [16:29:00<69:23:34, 206.80s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 20%|█▉        | 293/1500 [16:32:30<69:37:26, 207.66s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      " 20%|█▉        | 294/1500 [16:35:53<69:07:02, 206.32s/it]/home/lklimkiewicz/miniconda3/envs/alpaca-lora/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    base_model='decapoda-research/llama-7b-hf',\n",
    "    data_path='klima7/polish-prose',\n",
    "    resume_from_checkpoint='models/polpaca',\n",
    "    output_dir='models/alpaca-prose-0',\n",
    "    lora_target_modules=['q_proj','k_proj','v_proj','o_proj'],\n",
    "    lora_r=16,\n",
    "    num_epochs=100,\n",
    "    micro_batch_size=8,\n",
    "    cutoff_len=512,\n",
    "    group_by_length=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
