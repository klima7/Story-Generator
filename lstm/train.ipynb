{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "%autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/stud/piat/Story-Generator/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMTokenizer, RobertaModel\n",
    "\n",
    "from dataset import TextTrainDataset\n",
    "from callback import GenerateCallback\n",
    "from lstm import LstmTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.encode(\"witaj świecie\")\n",
    "# print(encoded)\n",
    "# decoded = tokenizer.decode(encoded)\n",
    "# print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import tokenize, pad\n",
    "\n",
    "\n",
    "class TextTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, seq_length, padding=(3, 30), remove_dialogs=True, remove_special_chars=False, lowercase=False, tqdm=False, cache_path=None, cache_ignore=False, min_line_length=0):\n",
    "        self.samplesset_path = dataset_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.padding = padding\n",
    "        self.remove_dialogs = remove_dialogs\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "        self.lowercase = lowercase\n",
    "        self.tqdm = tqdm\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_ignore = cache_ignore\n",
    "        self.min_line_length = min_line_length\n",
    "        \n",
    "        self.samples = self.__get_samples()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.__add_random_padding(self.samples[idx])\n",
    "        return np.array(sequence, dtype=np.int32), target\n",
    "    \n",
    "    def __get_samples(self):\n",
    "        if self.cache_path is None or self.cache_ignore or not Path(self.cache_path).exists():\n",
    "            samples = self.__create_samples()\n",
    "            self.__save_samples_to_cache(samples)\n",
    "            return samples\n",
    "        else:\n",
    "            return self.__load_samples_from_cache()\n",
    "        \n",
    "    def __load_samples_from_cache(self):\n",
    "        with open(self.cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def __save_samples_to_cache(self, samples):\n",
    "        Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(self.cache_path, 'wb') as f:\n",
    "            return pickle.dump(samples, f)\n",
    "        \n",
    "    def __create_samples(self):\n",
    "        paths = list(glob.glob(f'{self.samplesset_path}/**/*.txt', recursive=True))\n",
    "        random.shuffle(paths)\n",
    "        data = []\n",
    "        \n",
    "        if self.tqdm:\n",
    "            paths = tqdm(paths)\n",
    "        \n",
    "        for path in paths:\n",
    "            text = self.__read_text_from_file(path)\n",
    "            samples = self.__get_samples_from_text(text)\n",
    "            data.extend(samples)\n",
    "                \n",
    "        return data\n",
    "                \n",
    "    def __get_samples_from_text(self, text):\n",
    "        samples = []\n",
    "        tokenized = self.tokenizer.encode(text)[1:-1]\n",
    "        \n",
    "        start_idx = -self.seq_length + self.padding[0]\n",
    "        end_idx = len(tokenized) - self.seq_length - 1\n",
    "        \n",
    "        for idx in range(start_idx, end_idx):\n",
    "            sequence = tokenized[max(idx, 0) : idx+self.seq_length]\n",
    "            target = tokenized[idx+self.seq_length]\n",
    "            samples.append((sequence, target))\n",
    "            \n",
    "        return samples\n",
    "\n",
    "    def __add_random_padding(self, sample):\n",
    "        sequence, target = sample\n",
    "        sequence_len = min(random.randint(self.padding[0], self.padding[1]), self.seq_length)\n",
    "        pad_sequence = pad(sequence[:sequence_len], self.seq_length, pad_token=self.tokenizer.pad_token_id)\n",
    "        return pad_sequence, target\n",
    "\n",
    "    def __read_text_from_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = map(self.__preprocess_line, lines)\n",
    "            lines = filter(lambda line: len(line) > self.min_line_length, lines)\n",
    "            if self.remove_dialogs:\n",
    "                lines = self.__remove_dialogs(lines)\n",
    "            text = '\\n'.join(lines)\n",
    "            if self.remove_special_chars:\n",
    "                text = re.sub(r'[^a-ząćęłńóśźż.,!? \\n]', ' ', text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "\n",
    "    def __remove_dialogs(self, lines):\n",
    "        return filter(lambda line: not self.__is_dialog_line(line), lines)\n",
    "    \n",
    "    def __preprocess_line(self, line):\n",
    "        line = line.strip()\n",
    "        if self.lowercase:\n",
    "            line = line.lower()\n",
    "        return line\n",
    "        \n",
    "    @staticmethod\n",
    "    def __is_dialog_line(line):\n",
    "        return '—' in line or '–' in line or '-' in line or '„' in line or '\"' in line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1036/1036 [00:37<00:00, 27.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TextTrainDataset('../../data/training', tokenizer, seq_length=20, padding=(3, 70), lowercase=True, tqdm=True, cache_path='.cache/dataset', cache_ignore=True, remove_special_chars=True, min_line_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21570081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    # files\n",
    "    train_dataset_path='../../data/training/',\n",
    "    \n",
    "    # architecture\n",
    "    embedding_dim=300,\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=1024,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True,\n",
    "    \n",
    "    # training\n",
    "    lr=0.001,\n",
    "    seq_length=20,\n",
    "    padding=(3, 40),\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator.load_from_checkpoint('../../logs/version_21/checkpoints/epoch=43-step=161906.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
       "============================================================================================================================================\n",
       "LstmTextGenerator                        [512, 20]                 [512, 50560]              --                             --\n",
       "├─Embedding: 1-1                         [512, 20]                 [512, 20, 300]            15,168,000                  8.43%\n",
       "├─LSTM: 1-2                              [512, 20, 300]            [512, 20, 2048]           61,227,008                 34.02%\n",
       "├─Dropout: 1-3                           [512, 20, 2048]           [512, 20, 2048]           --                             --\n",
       "├─Linear: 1-4                            [512, 2048]               [512, 50560]              103,597,440                57.56%\n",
       "============================================================================================================================================\n",
       "Total params: 179,992,448\n",
       "Trainable params: 179,992,448\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 687.77\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.08\n",
       "Forward/backward pass size (MB): 399.44\n",
       "Params size (MB): 719.97\n",
       "Estimated Total Size (MB): 1119.49\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(512, 20),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent'],\n",
    "    dtypes=[torch.LongTensor],\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "generate_callback = GenerateCallback(\n",
    "    'Pewnego dnia czerwony kapturek szedł przez las z koszyczkiem jedzenia do swojej babci, która mieszkała w lesie. Śledził go jednak zły wilk, który chciał zjeść dziewczynkę.',\n",
    "    temperatures=[0.01, 0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "    length=200,\n",
    "    interval=1000\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_last=True,\n",
    "    every_n_train_steps=1000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    callbacks=[generate_callback, checkpoint_callback],\n",
    "    gradient_clip_val=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.hparams.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at ../../logs/version_21/checkpoints/epoch=43-step=161906.ckpt\n",
      "/home/lklimkiewicz/stud/piat/Story-Generator/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:243: UserWarning: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\"].\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | embed   | Embedding        | 15.2 M\n",
      "1 | lstm    | LSTM             | 61.2 M\n",
      "2 | dropout | Dropout          | 0     \n",
      "3 | fc      | Linear           | 103 M \n",
      "4 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "179 M     Trainable params\n",
      "0         Non-trainable params\n",
      "179 M     Total params\n",
      "719.970   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at ../../logs/version_21/checkpoints/epoch=43-step=161906.ckpt\n",
      "/home/lklimkiewicz/stud/piat/Story-Generator/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57:  56%|█████▌    | 1047/1876 [28:07<22:16,  1.61s/it, v_num=23]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lklimkiewicz/stud/piat/Story-Generator/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(generator, train_dataloaders=train_dataloader, ckpt_path='../../logs/version_21/checkpoints/epoch=43-step=161906.ckpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru i wesoły i głodny. ale nie mógł go nigdzie znaleźć, ale nie mógł go znaleźć, bo mu wstyd było przyznać się publicznie do powrotu, a on, nie mogąc się doczekać listu od chwili, poszedł do paryża, aby go nie rozwiązy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru', temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma dolinami, za siedmioma jeziorami, między innymi, a siódmym a, w której, w okolicy, nie było końca, nie było mowy o żywych głębiach greckich, których nie znano jeszcze żadnej, które by można było nazwać w'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i', temperature=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
