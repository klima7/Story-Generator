{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import TextTrainDataset\n",
    "from lstm import LstmTextGenerator\n",
    "from utils import tokenize, pad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_iterator(dir_path):\n",
    "    paths = list(glob.glob(f'{dir_path}/**/*.txt', recursive=True))\n",
    "    for path in paths:\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "            tokenized = tokenize(text, flatten=True)\n",
    "            yield tokenized\n",
    "            \n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    sentences_iterator('../../data/training/'),\n",
    "    max_tokens=80_000,\n",
    "    specials=['<PAD>']\n",
    ")\n",
    "\n",
    "vocab.set_default_index(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, '../../models/vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load('../../models/vocab.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextTrainDataset('../../data/training', vocab, seq_length=15, padding=(3, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3581485"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 204, 61, 215, 8355, 5049, 212, 1, 6584, 27665, 4, 7, 4789, 16, 43285],\n",
       " 2572)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[503]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    # files\n",
    "    vocabulary_path='../../models/vocab.pth',\n",
    "    train_dataset_path='../../data/training',\n",
    "    \n",
    "    # architecture\n",
    "    embedding_dim=300,\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=512,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True,\n",
    "    \n",
    "    # training\n",
    "    lr=0.001,\n",
    "    seq_length=20,\n",
    "    batch_size=512,\n",
    "    padding=(3, 50),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator.load_from_checkpoint(\n",
    "    '../../logs/version_84/checkpoints/epoch=29-step=209880.ckpt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
       "============================================================================================================================================\n",
       "LstmTextGenerator                        [512, 20]                 [512, 80000]              --                             --\n",
       "├─Embedding: 1-1                         [512, 20]                 [512, 20, 300]            24,000,000                 19.68%\n",
       "├─LSTM: 1-2                              [512, 20, 300]            [512, 20, 1024]           15,933,440                 13.07%\n",
       "├─Linear: 1-3                            [512, 1024]               [512, 80000]              82,000,000                 67.25%\n",
       "============================================================================================================================================\n",
       "Total params: 121,933,440\n",
       "Trainable params: 121,933,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 217.43\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.08\n",
       "Forward/backward pass size (MB): 436.14\n",
       "Params size (MB): 487.73\n",
       "Estimated Total Size (MB): 923.96\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(512, 20),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent'],\n",
    "    dtypes=[torch.LongTensor],\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at ../../logs/version_84/checkpoints/epoch=29-step=209880.ckpt\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:337: UserWarning: The dirpath has changed from '../../logs/version_84/checkpoints' to '../../logs/version_85/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | vocab | Vocab     | 0     \n",
      "1 | embed | Embedding | 24.0 M\n",
      "2 | lstm  | LSTM      | 15.9 M\n",
      "3 | fc    | Linear    | 82.0 M\n",
      "------------------------------------\n",
      "121 M     Trainable params\n",
      "0         Non-trainable params\n",
      "121 M     Total params\n",
      "487.734   Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 11.76 GiB total capacity; 557.59 MiB already allocated; 76.12 MiB free; 562.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(generator, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../../logs/version_84/checkpoints/epoch=29-step=209880.ckpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:926\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[39m# restore optimizers, etc.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: restoring training state\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 926\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49mrestore_training_state()\n\u001b[1;32m    928\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:297\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_training_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n\u001b[1;32m    296\u001b[0m     \u001b[39m# restore optimizers and schedulers state\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_optimizers_and_schedulers()\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:366\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_optimizers_and_schedulers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moptimizer_states\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loaded_checkpoint:\n\u001b[1;32m    362\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    363\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTrying to restore optimizer state but checkpoint contains only the model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         )\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_optimizers()\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlr_schedulers\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loaded_checkpoint:\n\u001b[1;32m    369\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to restore learning rate scheduler state but checkpoint contains only the model.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m This is probably due to `ModelCheckpoint.save_weights_only` being set to `True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m     )\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:381\u001b[0m, in \u001b[0;36m_CheckpointConnector.restore_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# restore the optimizers\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mload_optimizer_state_dict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loaded_checkpoint)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:356\u001b[0m, in \u001b[0;36mStrategy.load_optimizer_state_dict\u001b[0;34m(self, checkpoint)\u001b[0m\n\u001b[1;32m    354\u001b[0m optimizer_states \u001b[39m=\u001b[39m checkpoint[\u001b[39m\"\u001b[39m\u001b[39moptimizer_states\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    355\u001b[0m \u001b[39mfor\u001b[39;00m optimizer, opt_state \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers, optimizer_states):\n\u001b[0;32m--> 356\u001b[0m     optimizer\u001b[39m.\u001b[39;49mload_state_dict(opt_state)\n\u001b[1;32m    357\u001b[0m     _optimizer_to_device(optimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/optim/optimizer.py:423\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m id_map:\n\u001b[1;32m    422\u001b[0m     param \u001b[39m=\u001b[39m id_map[k]\n\u001b[0;32m--> 423\u001b[0m     state[param] \u001b[39m=\u001b[39m cast(param, v)\n\u001b[1;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m     state[k] \u001b[39m=\u001b[39m v\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/optim/optimizer.py:410\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict.<locals>.cast\u001b[0;34m(param, value, key)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m    409\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: cast(param, v, key\u001b[39m=\u001b[39mk) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m value\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    411\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, container_abcs\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(value)(cast(param, v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/optim/optimizer.py:410\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m    409\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: cast(param, v, key\u001b[39m=\u001b[39;49mk) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m value\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    411\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, container_abcs\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(value)(cast(param, v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/optim/optimizer.py:407\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict.<locals>.cast\u001b[0;34m(param, value, key)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mis_floating_point():\n\u001b[1;32m    406\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mto(param\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 407\u001b[0m         value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(param\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[1;32m    409\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 11.76 GiB total capacity; 557.59 MiB already allocated; 76.12 MiB free; 562.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.fit(generator, ckpt_path='../../logs/version_84/checkpoints/epoch=29-step=209880.ckpt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma najdrobniejszymi melvill klaro klapsa zagrażało hip rycerskim świątobliwości dziurawych bryka piekielnego szlachecką spokojnym znalazłam atved uczciwej roztaczały mmerung opętało rozeznanie sposobności kmiotek czynu cudzoziemcami lottie założyciela potargane kinie samowarek ostrzału wyśmiewały udawaj kataryniarz wytoczył trzcinę poczytaj baranki gwarny gębą wytężonym urosnę allerdings wykrzyki majstrze załamuje firmą poniewierce niezłomnym jadąc tęgo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma', temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem powrotem. - cóż to jest. kundel aż z dala. - super mały góra, albo zachować ładnie piskiem orzech, autorka l. mróz - cieślik wierszyk z obrazkiem - bajeczki - pręgi, uwaga, sio. ja gotowy - - wnuczek coś złoży! mamo'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem', temperature=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
