{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from transformers import XLMTokenizer, RobertaModel\n",
    "\n",
    "from dataset import TextTrainDataset\n",
    "from callback import GenerateCallback\n",
    "from lstm import LstmTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((10))\n",
    "b = torch.ones((10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.cat([a, b], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPECIAL_TOKENS_ATTRIBUTES', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_tokens', '_additional_special_tokens', '_auto_class', '_batch_encode_plus', '_batch_prepare_for_model', '_bos_token', '_call_one', '_cls_token', '_convert_id_to_token', '_convert_token_to_id', '_convert_token_to_id_with_added_voc', '_create_repo', '_create_trie', '_decode', '_decode_use_source_tokenizer', '_encode_plus', '_eos_token', '_eventual_warn_about_too_long_sequence', '_eventually_correct_t5_max_length', '_from_pretrained', '_get_files_timestamps', '_get_padding_truncation_strategies', '_in_target_context_manager', '_mask_token', '_pad', '_pad_token', '_pad_token_type_id', '_processor_class', '_save_pretrained', '_sep_token', '_set_processor_class', '_switch_to_input_mode', '_switch_to_target_mode', '_tokenize', '_unk_token', '_upload_modified_files', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder', 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens', 'all_special_tokens_extended', 'as_target_tokenizer', 'batch_decode', 'batch_encode_plus', 'bos_token', 'bos_token_id', 'bpe', 'bpe_ranks', 'build_inputs_with_special_tokens', 'cache', 'cache_moses_punct_normalizer', 'cache_moses_tokenizer', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string', 'create_token_type_ids_from_sequences', 'decode', 'decoder', 'deprecation_warnings', 'do_lower_case', 'do_lowercase_and_remove_accent', 'encode', 'encode_plus', 'encoder', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_special_tokens_mask', 'get_vocab', 'id2lang', 'init_inputs', 'init_kwargs', 'is_fast', 'ja_tokenize', 'ja_word_tokenizer', 'lang2id', 'lang_with_custom_tokenizer', 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes', 'model_input_names', 'model_max_length', 'moses_pipeline', 'moses_punct_norm', 'moses_tokenize', 'name_or_path', 'num_special_tokens_to_add', 'pad', 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_for_tokenization', 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub', 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token', 'sep_token_id', 'slow_tokenizer_class', 'sm', 'special_tokens_map', 'special_tokens_map_extended', 'tokenize', 'tokens_trie', 'truncate_sequences', 'truncation_side', 'unique_no_split_tokens', 'unk_token', 'unk_token_id', 'verbose', 'vocab_files_names', 'vocab_size', 'zh_word_tokenizer']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 357, 23008, 945, 1]\n",
      "witaj świecie\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"witaj świecie\")\n",
    "print(encoded)\n",
    "decoded = tokenizer.decode(encoded, skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import tokenize, pad\n",
    "\n",
    "\n",
    "class TextTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, seq_length, padding=(3, 30), remove_dialogs=True, remove_special_chars=False, lowercase=False, tqdm=False, cache_path=None, cache_ignore=False, min_line_length=0):\n",
    "        self.samplesset_path = dataset_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.padding = padding\n",
    "        self.remove_dialogs = remove_dialogs\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "        self.lowercase = lowercase\n",
    "        self.tqdm = tqdm\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_ignore = cache_ignore\n",
    "        self.min_line_length = min_line_length\n",
    "        \n",
    "        self.samples = self.__get_samples()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.__add_random_padding(self.samples[idx])\n",
    "        return np.array(sequence, dtype=np.int32), target\n",
    "    \n",
    "    def __get_samples(self):\n",
    "        if self.cache_path is None or self.cache_ignore or not Path(self.cache_path).exists():\n",
    "            samples = self.__create_samples()\n",
    "            self.__save_samples_to_cache(samples)\n",
    "            return samples\n",
    "        else:\n",
    "            return self.__load_samples_from_cache()\n",
    "        \n",
    "    def __load_samples_from_cache(self):\n",
    "        with open(self.cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def __save_samples_to_cache(self, samples):\n",
    "        Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(self.cache_path, 'wb') as f:\n",
    "            return pickle.dump(samples, f)\n",
    "        \n",
    "    def __create_samples(self):\n",
    "        paths = list(glob.glob(f'{self.samplesset_path}/**/*.txt', recursive=True))\n",
    "        random.shuffle(paths)\n",
    "        data = []\n",
    "        \n",
    "        if self.tqdm:\n",
    "            paths = tqdm(paths)\n",
    "        \n",
    "        for path in paths:\n",
    "            text = self.__read_text_from_file(path)\n",
    "            samples = self.__get_samples_from_text(text)\n",
    "            data.extend(samples)\n",
    "                \n",
    "        return data\n",
    "                \n",
    "    def __get_samples_from_text(self, text):\n",
    "        samples = []\n",
    "        tokenized = self.tokenizer.encode(text)[1:-1]\n",
    "        \n",
    "        start_idx = -self.seq_length + self.padding[0]\n",
    "        end_idx = len(tokenized) - self.seq_length - 1\n",
    "        \n",
    "        for idx in range(start_idx, end_idx):\n",
    "            sequence = tokenized[max(idx, 0) : idx+self.seq_length]\n",
    "            target = tokenized[idx+self.seq_length]\n",
    "            samples.append((sequence, target))\n",
    "            \n",
    "        return samples\n",
    "\n",
    "    def __add_random_padding(self, sample):\n",
    "        sequence, target = sample\n",
    "        sequence_len = min(random.randint(self.padding[0], self.padding[1]), self.seq_length)\n",
    "        pad_sequence = pad(sequence[:sequence_len], self.seq_length, pad_token=self.tokenizer.pad_token_id)\n",
    "        return pad_sequence, target\n",
    "\n",
    "    def __read_text_from_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = map(self.__preprocess_line, lines)\n",
    "            lines = filter(lambda line: len(line) > self.min_line_length, lines)\n",
    "            if self.remove_dialogs:\n",
    "                lines = self.__remove_dialogs(lines)\n",
    "            text = '\\n'.join(lines)\n",
    "            if self.remove_special_chars:\n",
    "                text = re.sub(r'[^a-ząćęłńóśźż.,!? \\n]', ' ', text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "\n",
    "    def __remove_dialogs(self, lines):\n",
    "        return filter(lambda line: not self.__is_dialog_line(line), lines)\n",
    "    \n",
    "    def __preprocess_line(self, line):\n",
    "        line = line.strip()\n",
    "        if self.lowercase:\n",
    "            line = line.lower()\n",
    "        return line\n",
    "        \n",
    "    @staticmethod\n",
    "    def __is_dialog_line(line):\n",
    "        return '—' in line or '–' in line or '-' in line or '„' in line or '\"' in line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1036/1036 [00:33<00:00, 31.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TextTrainDataset('../../data/training', tokenizer, seq_length=20, padding=(3, 70), lowercase=True, tqdm=True, cache_path='.cache/dataset', cache_ignore=True, remove_special_chars=True, min_line_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3840943"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    # files\n",
    "    train_dataset_path='../../data/training/',\n",
    "    \n",
    "    # architecture\n",
    "    embedding_dim=300,\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=1024,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True,\n",
    "    \n",
    "    # training\n",
    "    lr=0.001,\n",
    "    seq_length=20,\n",
    "    padding=(3, 40),\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generator = LstmTextGenerator.load_from_checkpoint('../../logs/version_16/checkpoints/epoch=19-step=46952.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
       "============================================================================================================================================\n",
       "LstmTextGenerator                        [512, 20]                 [512, 50560]              --                             --\n",
       "├─Embedding: 1-1                         [512, 20]                 [512, 20, 300]            15,168,000                  8.43%\n",
       "├─LSTM: 1-2                              [512, 20, 300]            [512, 20, 2048]           61,227,008                 34.02%\n",
       "├─Dropout: 1-3                           [512, 20, 2048]           [512, 20, 2048]           --                             --\n",
       "├─Linear: 1-4                            [512, 2048]               [512, 50560]              103,597,440                57.56%\n",
       "============================================================================================================================================\n",
       "Total params: 179,992,448\n",
       "Trainable params: 179,992,448\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 687.77\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.08\n",
       "Forward/backward pass size (MB): 399.44\n",
       "Params size (MB): 719.97\n",
       "Estimated Total Size (MB): 1119.49\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(512, 20),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent'],\n",
    "    dtypes=[torch.LongTensor],\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "generate_callback = GenerateCallback(\n",
    "    'Pewnego dnia czerwony kapturek szedł przez las z koszyczkiem jedzenia do swojej babci, która mieszkała w lesie. Śledził go jednak zły wilk, który chciał zjeść dziewczynkę.',\n",
    "    temperatures=[0.01, 0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "    length=200,\n",
    "    interval=2000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    callbacks=[generate_callback],\n",
    "    gradient_clip_val=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.hparams.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | embed   | Embedding        | 15.2 M\n",
      "1 | lstm    | LSTM             | 61.2 M\n",
      "2 | dropout | Dropout          | 0     \n",
      "3 | fc      | Linear           | 103 M \n",
      "4 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "179 M     Trainable params\n",
      "0         Non-trainable params\n",
      "179 M     Total params\n",
      "719.970   Total estimated model params size (MB)\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  59%|█████▉    | 4424/7502 [35:38<24:47,  2.07it/s, v_num=17]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(generator, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru, ale nie miał pojęcia, że nie ma w domu. Ktoś musiał wracać do domu, ale nie mógł się z nią bawić. Nie był to jednak zwykły kurczak, bo miał nadzieję, że nie ma na to żadnego dowody.- Nie ma'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru', temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru czekoladowego piernika, a jego mama piekła pyszne jabłka. Nie namyślając się długo, posadziła go na stole, aby poczęstować go ciastkami. Didi przyglądał się pracy, ale szybko stwierdził, że Alfred nie lubi, bo nie widział nigdzie. Było to miejsce, gdzie było ciepło i przyjemnie było dostrzec, czy tam zwierzątka nie widać było, gdzie indziej i w nocy. Leżał oddychając spokojnie i spokojnie. Tylko brzuszek podnosił mu się i opadał miarowo. Ten biały, puszysty i biały, brzuszek i buziak. Natalia wzięła urlop, by się z nim bawić, bo przecież nie chciała, żeby jej rodzice nie wiedzieli, jak się bawić. Nie podobało jej się, że Kasia jest smutna i nie lubiła, gdy już wszyscy się bawili. Mama, tata i trójka, jak to dzieci, nieraz szedł do przodu, a gdy do domu wrócił tata, który miał trafić do domu, miał wrażenie, że to nie jego wina. Chciał powiedzieć, że nastąpiło mu coś zupełnie nieoczekiwanego. Niestety, nie umiał pływać, tylko jego mikstura nie mogła się powstrzymać. W pewnym momencie z oddali usłyszał przeraźliwy huk, a w powietrze wystrzelił słup wody. - Piękny las! - zawołał. - To musi być statek. Nie ma nóg. A to co? - Sprytne! - Ale czy to prawda, że to prawda? - Takie to, co mają do powiedzenia. - No to dość dziwna przypadłość -'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru', temperature=0.3, length=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
