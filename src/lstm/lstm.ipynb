{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchinfo import summary\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, wv, seq_length=10, target_length=1, padding_factor=20, padding_limit=3, epoch_size=100_000):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.wv = wv\n",
    "        self.seq_length = seq_length\n",
    "        self.target_length = target_length\n",
    "        self.padding_factor = padding_factor\n",
    "        self.padding_limit = padding_limit\n",
    "        self.epoch_size = epoch_size\n",
    "        \n",
    "        self.paths = list(glob.glob(f'{dataset_path}/**/*.txt', recursive=True))\n",
    "        self.distribution = self.__construct_distribution()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.sum(self.distribution)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        distribution = np.array(self.distribution)\n",
    "        indexes = np.arange(len(self.paths))\n",
    "        \n",
    "        while distribution.any():\n",
    "            text_index = np.random.choice(indexes, p=distribution/distribution.sum())\n",
    "            distribution[text_index] -= 1\n",
    "            text = self.__get_text(self.paths[text_index])\n",
    "            yield self.__get_sample_from_text(text)\n",
    "    \n",
    "    def __get_text(self, path):\n",
    "        with open(path) as f:\n",
    "            text = f.read()\n",
    "            return text.split(' ')[:-1]\n",
    "    \n",
    "    def __construct_distribution(self):\n",
    "        lengths = self.__get_files_lengths()\n",
    "        distribution = np.ceil(lengths / lengths.sum() * self.epoch_size).astype(np.int32)\n",
    "        return distribution\n",
    "    \n",
    "    def __get_files_lengths(self):\n",
    "        lengths = []\n",
    "        for path in self.paths:\n",
    "            text = self.__get_text(path)\n",
    "            lengths.append(len(text))\n",
    "        return np.array(lengths)\n",
    "    \n",
    "    def __get_sample_from_text(self, text):\n",
    "        length = min(random.randint(self.padding_limit, self.padding_factor), self.seq_length, len(text)-self.target_length)\n",
    "        start_idx = random.randint(0, len(text)-length-self.target_length)\n",
    "        \n",
    "        seq = self.__padd(text[start_idx : start_idx+length])\n",
    "        target = text[start_idx+length:start_idx+length+self.target_length]\n",
    "        \n",
    "        seq_embed = self.wv[seq]\n",
    "        target_embed = self.wv[target]\n",
    "        \n",
    "        return np.array(seq_embed), np.array(target_embed)\n",
    "    \n",
    "    def __padd(self, text):\n",
    "        if len(text) < self.seq_length:\n",
    "            padding = ['kot']*(self.seq_length-len(text))\n",
    "            text = padding + text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv = Word2Vec.load('../../models/word2vec/word2vec').wv\n",
    "\n",
    "# dataset = TextTrainDataset('../../data/training_prepared/', wv)\n",
    "\n",
    "# dl = DataLoader(\n",
    "#     dataset,\n",
    "#     num_workers=16,\n",
    "#     batch_size=64,\n",
    "#     prefetch_factor=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTextGenerator(LightningModule):\n",
    "    \n",
    "    VALIDATION_PROMPTS = [\n",
    "        'Pewnego dnia czerwony kapturek szedł przez las z koszyczkiem jedzenia do swojej babci, która mieszkała w lesie. Śledził go jednak zły wilk, który chciał zjeść dziewczynkę.',\n",
    "        'Za siedmioma górami i za siedmioma rzekami było sobie królestwo, w którym mieszkała księżniczka',\n",
    "        'Dawno, dawno temu żył pewien chłopiec',\n",
    "    ]\n",
    "    \n",
    "    VALIDATION_TEMPERATURES = [0.01, 0.2, 0.3, 0.5, 0.7]\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        \n",
    "        # files\n",
    "        vocabulary_path,\n",
    "        train_file_path,\n",
    "        \n",
    "        # architecture\n",
    "        embedding_dim=100,\n",
    "        lstm_layers=1,\n",
    "        lstm_dropout=0,\n",
    "        lstm_hidden_size=100,\n",
    "        dropout=0.2,\n",
    "        bidirectional=False,\n",
    "        \n",
    "        # training process\n",
    "        batch_size=64,\n",
    "        seq_length=10, \n",
    "        target_length=10,\n",
    "        target_weight_decrease=1.0,\n",
    "        padding_factor=20,\n",
    "        padding_limit=3,\n",
    "        epoch_size=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.vocabulary = torch.load(self.hparams.vocabulary_path)\n",
    "        self.vocabulary.append_token('<pad>')\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            len(self.vocabulary),\n",
    "            self.hparams.embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.hparams.embedding_dim,\n",
    "            hidden_size=self.hparams.lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.lstm_dropout,\n",
    "            bidirectional=self.hparams.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear((2 if self.hparams.bidirectional else 1)*self.hparams.lstm_hidden_size, len(self.vocabulary))\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def generate(self, prompt, length=50, temperature=0.5):\n",
    "        generated = prompt\n",
    "        prompt = self.__preprocess_prompt(prompt)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            input_tensor = torch.unsqueeze(torch.tensor(prompt, device=self.device), dim=0)\n",
    "            next_word_logits = self(input_tensor)[0]\n",
    "            word_idx = self.__get_word_from_logits(next_word_logits, temperature)\n",
    "            prompt = prompt[1:] + [word_idx]\n",
    "            \n",
    "            word = self.vocabulary.lookup_token(word_idx)\n",
    "            if generated[-1] in '.!?':\n",
    "                word = word.capitalize()\n",
    "            if word not in list('.!?,'):\n",
    "                generated += ' '\n",
    "            generated += word\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def __get_word_from_logits(self, next_word_logits, temperature=0.5):\n",
    "        scaled_logits = next_word_logits / temperature\n",
    "        adjusted_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "        return next_word_index\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        \n",
    "    def training_step(self, batch, batch_no):\n",
    "        texts, targets = batch\n",
    "        loss = 0\n",
    "        weight = 1.0\n",
    "        \n",
    "        for i in range(self.hparams.target_length):\n",
    "            predicted = self.forward(texts)\n",
    "            loss_part = self.loss(predicted, targets[:, i])\n",
    "            loss += weight * loss_part\n",
    "            weight *= self.hparams.target_weight_decrease\n",
    "            \n",
    "            words_idx = predicted.argmax(dim=-1)\n",
    "            texts = torch.cat((texts[:, 1:], words_idx.unsqueeze(1)), axis=1)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        tensorboard = self.logger.experiment\n",
    "        \n",
    "        for no, validation_prompt in enumerate(self.VALIDATION_PROMPTS):\n",
    "            for temperature in self.VALIDATION_TEMPERATURES:\n",
    "                text = self.generate(validation_prompt, length=300, temperature=temperature)\n",
    "                tensorboard.add_text(f'text_{no}_{temperature}', text, global_step=self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TextTrainDataset(\n",
    "            self.hparams.train_file_path,\n",
    "            ...,\n",
    "            seq_length=self.hparams.seq_length,\n",
    "            target_length=self.hparams.target_length,\n",
    "            padding_factor=self.hparams.padding_factor,\n",
    "            padding_limit=self.hparams.padding_limit,\n",
    "            epoch_size=self.hparams.epoch_size,\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=12\n",
    "        )\n",
    "        \n",
    "    def __preprocess_prompt(self, prompt):\n",
    "        tokenized = self.__tokenize(prompt)\n",
    "        words_idx = self.vocabulary(tokenized)\n",
    "        words_idx = [idx for idx in words_idx if idx != -1]\n",
    "        padding = [0]*(max(self.hparams.seq_length-len(prompt), 0))\n",
    "        prompt = padding + words_idx\n",
    "        return prompt\n",
    "    \n",
    "    def __tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-ząćęłńóśźż.,!?\\- ]', ' ', text)\n",
    "        text = re.sub(r'([,-.!?])', ' \\\\1 ', text)\n",
    "        text = [word for word in text.split(' ') if word]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    # files\n",
    "    train_file_path='../../data/binary_texts/ebooks17k.pickle',\n",
    "    vocabulary_path='../../models/vocabulary.pth',\n",
    "    \n",
    "    # architecture\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=100,\n",
    "    dropout=0.2,\n",
    "    bidirectional=False,\n",
    "    \n",
    "    # training\n",
    "    seq_length=25,\n",
    "    target_length=1,\n",
    "    target_weight_decrease=0.7,\n",
    "    batch_size=128,\n",
    "    padding_factor=100,\n",
    "    padding_limit=4,\n",
    "    epoch_size=2_000_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# generator = LstmTextGenerator.load_from_checkpoint(\n",
    "#     '/home/klima7/studies/piat/Story-Generator/logs/version_3/checkpoints/epoch=27-step=110516.ckpt',\n",
    "#     batch_size=3048,\n",
    "#     epoch_size=6_000_000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
       "============================================================================================================================================\n",
       "LstmTextGenerator                        [64, 20]                  [64, 150001]              --                             --\n",
       "├─Embedding: 1-1                         [64, 20]                  [64, 20, 100]             15,000,100                 49.62%\n",
       "├─LSTM: 1-2                              [64, 20, 100]             [64, 20, 100]             80,800                      0.27%\n",
       "├─Dropout: 1-3                           [64, 20, 100]             [64, 20, 100]             --                             --\n",
       "├─Linear: 1-4                            [64, 100]                 [64, 150001]              15,150,101                 50.11%\n",
       "============================================================================================================================================\n",
       "Total params: 30,231,001\n",
       "Trainable params: 30,231,001\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.03\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 78.85\n",
       "Params size (MB): 120.92\n",
       "Estimated Total Size (MB): 199.78\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(64, 20),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent'],\n",
    "    dtypes=[torch.LongTensor],\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | vocabulary | Vocab            | 0     \n",
      "1 | embedding  | Embedding        | 15.0 M\n",
      "2 | lstm       | LSTM             | 242 K \n",
      "3 | fc         | Linear           | 15.2 M\n",
      "4 | dropout    | Dropout          | 0     \n",
      "5 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "30.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.4 M    Total params\n",
      "121.570   Total estimated model params size (MB)\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  36%|███▋      | 5699/15691 [01:45<03:05, 53.90it/s, v_num=2] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma zimy, wiadro były balony i w książkach na fachu przystrojone się aż lekko ptak się spotkały przez by węgiel nie złożę wszystkie zwierzęta na stałe zdrowie cicho przepisane znaczy fabryczne ich głośne groszy dzwonek co się wziął wziął dba z, ten dzień miesiąc szybko zaproszę pisać diety pięknie'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma', temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem powrotem. - cóż to jest. kundel aż z dala. - super mały góra, albo zachować ładnie piskiem orzech, autorka l. mróz - cieślik wierszyk z obrazkiem - bajeczki - pręgi, uwaga, sio. ja gotowy - - wnuczek coś złoży! mamo'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem', temperature=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
