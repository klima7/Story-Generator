{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchinfo import summary\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [00:00, 854.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "text_data = [\"Hello, how are you?\", \"I am doing well.\", \"Hello, how are you?\"]\n",
    "\n",
    "ls = LineSentence('../../data/line_sentence/demo.txt')\n",
    "vocab = build_vocab_from_iterator(tqdm(ls), max_tokens=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['ma',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 4, 'you?': 3, 'how': 2, 'are': 1, 'Hello,': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'are', 'how', 'you?', 'I']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39msplit()  \u001b[39m# Split text into a list of tokens\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Initialize the Field object\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m text_field \u001b[39m=\u001b[39m torchtext\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mField(tokenize\u001b[39m=\u001b[39mtokenizer, lower\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Process your text data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m text_data \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mHello, how are you?\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mI am doing well.\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()  # Split text into a list of tokens\n",
    "\n",
    "# Initialize the Field object\n",
    "text_field = torchtext.data.Field(tokenize=tokenizer, lower=True)\n",
    "\n",
    "# Process your text data\n",
    "text_data = [\"Hello, how are you?\", \"I am doing well.\"]\n",
    "processed_data = [torchtext.data.Example.fromlist([text], [('text', text_field)]) for text in text_data]\n",
    "\n",
    "# # Build the vocabulary\n",
    "print(processed_data)\n",
    "text_field.build_vocab(processed_data)\n",
    "\n",
    "# # Convert text to numerical indices\n",
    "# numerical_data = torchtext.data.Batch(processed_data, [('text', text_field)])\n",
    "\n",
    "# # Access the tokenized text\n",
    "# tokenized_text = numerical_data.text\n",
    "\n",
    "# print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, text_file_path, vocabulary, seq_length=10):\n",
    "        self.text_file_path = text_file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.__len = self.__count_lines_in_file()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.__len\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for text in LineSentence(self.text_file_path):\n",
    "            if len(text) < 2: continue\n",
    "            start_idx = random.randint(-self.seq_length+1, len(text)-self.seq_length-1)\n",
    "            cropped_text = text[max(start_idx, 0) : start_idx+self.seq_length]\n",
    "            cropped_text = self.__padd(cropped_text)\n",
    "            target = text[start_idx+self.seq_length]\n",
    "            yield cropped_text, target\n",
    "            \n",
    "    def __padd(self, text):\n",
    "        if len(text) < self.seq_length:\n",
    "            padding = ['<pad>']*(self.seq_length-len(text))\n",
    "            text = padding + text\n",
    "        return text\n",
    "            \n",
    "    def __count_lines_in_file(self):\n",
    "        with open(self.text_file_path) as f:\n",
    "            return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextValidationDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, text_file_path):\n",
    "        self.text_file_path = text_file_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for text in LineSentence(self.text_file_path):\n",
    "            yield ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTextGenerator(LightningModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 # files\n",
    "                 train_file_path,\n",
    "                 val_file_path,\n",
    "                 \n",
    "                 # training process\n",
    "                 seq_length=10, \n",
    "                 batch_size=64,\n",
    "                 \n",
    "                 # architecture\n",
    "                 vocab_size=100_000,\n",
    "                 embedding_dim=100,\n",
    "                 lstm_layers=1,\n",
    "                 lstm_dropout=0,\n",
    "                 lstm_hidden_size=100,\n",
    "                 dropout=0.2,\n",
    "                 bidirectional=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.vocabulary = self.__get_vocabulary()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            len(self.vocabulary),\n",
    "            self.hparams.embedding_dim\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=100,\n",
    "            hidden_size=self.hparams.lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.lstm_dropout,\n",
    "            bidirectional=self.hparams.bidirectional\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear((2 if self.hparams.bidirectional else 1)*self.hparams.lstm_hidden_size, len(self.vocabulary))\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def __get_vocabulary(self):\n",
    "        ls = LineSentence(self.hparams.train_file_path)\n",
    "        return build_vocab_from_iterator(ls, max_tokens=self.hparams.vocab_size, specials=['<PAD>'])\n",
    "        \n",
    "    def generate(self, prompt, length=50, temperature=0.5):\n",
    "        generated = prompt\n",
    "        prompt = self.__preprocess_prompt(prompt)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            embedded_prompt = self.vocabulary(prompt)\n",
    "            embedded_prompt = torch.tensor(embedded_prompt, device=self.device)\n",
    "            next_word_logits = self(torch.unsqueeze(embedded_prompt, dim=0))[0]\n",
    "            word = self.__get_word_from_logits(next_word_logits, temperature)\n",
    "            prompt = prompt[1:] + [word]\n",
    "            \n",
    "            if word not in list('.!?,'):\n",
    "                generated += ' '\n",
    "            generated += word\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def __get_word_from_logits(self, next_word_logits, temperature=0.5):\n",
    "        scaled_logits = next_word_logits / temperature\n",
    "        adjusted_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "        next_word = self.vocabulary.get_itos()[next_word_index]\n",
    "        return next_word\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        \n",
    "    def training_step(self, batch, batch_no):\n",
    "        text, target = batch\n",
    "        text = self.vocabulary(text)\n",
    "        target = self.vocabulary[target]\n",
    "        predicted = self.forward(text)\n",
    "        loss = self.loss(predicted, target)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_no):\n",
    "        prompt = batch[0]\n",
    "        tensorboard = self.logger.experiment\n",
    "        for temperature in [1, 0.5, 0.2, 0.1, 0.01]:\n",
    "            generated = self.generate(prompt, length=100, temperature=temperature)\n",
    "            tensorboard.add_text(f'val_generated_{temperature}_{batch_no}', generated, global_step=self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TextTrainDataset(\n",
    "            self.hparams.train_file_path,\n",
    "            self.vocabulary.get_stoi(),\n",
    "            self.hparams.seq_length,\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            # num_workers=24\n",
    "        )\n",
    "        \n",
    "    # def val_dataloader(self):\n",
    "    #     dataset = TextValidationDataset(\n",
    "    #         self.hparams.val_file_path,\n",
    "    #     )\n",
    "        \n",
    "    #     return DataLoader(\n",
    "    #         dataset=dataset,\n",
    "    #         batch_size=1\n",
    "    #     )\n",
    "        \n",
    "    def __preprocess_prompt(self, prompt):\n",
    "        prompt = prompt.lower().strip()\n",
    "        prompt = re.sub(r'[^a-ząćęłńóśźż.,!? ]', '', prompt)\n",
    "        prompt = prompt.replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ').replace(',', ' , ')\n",
    "        prompt = prompt.split()\n",
    "        prompt = [word for word in prompt if word in self.vocabulary]\n",
    "        padding = ['<pad>']*(max(self.hparams.seq_length-len(prompt), 0))\n",
    "        prompt = padding + prompt\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    check_val_every_n_epoch=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    train_file_path='../../data/line_sentence/demo.txt',\n",
    "    val_file_path='../../data/line_sentence/texts_validation.txt',\n",
    "    seq_length=25,\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=100,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(64, 20),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | vocabulary | Vocab            | 0     \n",
      "1 | embedding  | Embedding        | 10.0 M\n",
      "2 | lstm       | LSTM             | 644 K \n",
      "3 | fc         | Linear           | 20.1 M\n",
      "4 | dropout    | Dropout          | 0     \n",
      "5 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "30.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.7 M    Total params\n",
      "122.979   Total estimated model params size (MB)\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma lasami jednego dłonie życie. stanie mi więc? chyba kiedy pozostać odparł, sir i się od wszystkich czasu tę całą łaskę krew. ślad jak wobec mnie cię dostał. leży od. gdzie nie pozostanie obecny przy stanie, lecz i pies panu nie o zachód, ale'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma lasami', temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma lasami. a więc z tego będę, a nie jestem spotkamy. bądź zdrów, gdyż nie wiadomo, kim nie się. a więc nie wiadomo, kim nie powinienem, aby nie. nie mogę mi sobie, gdyż nie wiadomo o twoją. nie ma zamiaru mi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma lasami', temperature=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
