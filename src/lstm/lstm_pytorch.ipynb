{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchinfo import summary\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTrainDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, text_file_path, w2v, seq_length=10):\n",
    "        self.text_file_path = text_file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.w2v = w2v\n",
    "        self.__len = self.__count_lines_in_file()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for text in LineSentence(self.text_file_path):\n",
    "            if len(text) < 2: continue\n",
    "            for start_idx in range(-self.seq_length+1, len(text)-self.seq_length-1):\n",
    "                cropped_text = text[max(start_idx, 0) : start_idx+self.seq_length]\n",
    "                cropped_text = self.__padd(cropped_text)\n",
    "                target = text[start_idx+self.seq_length]\n",
    "                yield self.w2v.wv[cropped_text], self.w2v.wv.key_to_index[target]\n",
    "            \n",
    "    def __padd(self, text):\n",
    "        if len(text) < self.seq_length:\n",
    "            padding = ['<pad>']*(self.seq_length-len(text))\n",
    "            text = padding + text\n",
    "        return text\n",
    "            \n",
    "    def __count_lines_in_file(self):\n",
    "        with open(self.text_file_path) as f:\n",
    "            return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextValidationDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, text_file_path):\n",
    "        self.text_file_path = text_file_path\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for text in LineSentence(self.text_file_path):\n",
    "            yield ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTextGenerator(LightningModule):\n",
    "    \n",
    "    def __init__(self, text_file_path, val_file_path, word2vec_path, seq_length=10, \n",
    "                 batch_size=64, seed=42, lstm_layers=1, lstm_dropout=0, lstm_hidden_size=100,\n",
    "                 dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.w2v = Word2Vec.load(self.hparams.word2vec_path)\n",
    "        np.random.seed(seed)\n",
    "        self.w2v.wv['<pad>'] = np.random.rand(100)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=100,\n",
    "            hidden_size=self.hparams.lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.lstm_dropout,\n",
    "            bidirectional=self.hparams.bidirectional\n",
    "        )\n",
    "        self.fc = nn.Linear(2*self.hparams.lstm_hidden_size, len(self.w2v.wv))\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.hparams.dropout)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def generate(self, prompt, length=50, temperature=0.5):\n",
    "        generated = prompt\n",
    "        prompt = self.__preprocess_prompt(prompt)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            embedded_prompt = self.w2v.wv[prompt]\n",
    "            embedded_prompt = torch.tensor(embedded_prompt, device=self.device)\n",
    "            next_word_logits = self(torch.unsqueeze(embedded_prompt, dim=0))[0]\n",
    "            word = self.__get_word_from_logits(next_word_logits, temperature)\n",
    "            prompt = prompt[1:] + [word]\n",
    "            \n",
    "            if word not in list('.!?,'):\n",
    "                generated += ' '\n",
    "            generated += word\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def __get_word_from_logits(self, next_word_logits, temperature=0.5):\n",
    "        scaled_logits = next_word_logits / temperature\n",
    "        adjusted_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "        next_word = self.w2v.wv.index_to_key[next_word_index]\n",
    "        return next_word\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "        \n",
    "    def training_step(self, batch, batch_no):\n",
    "        text, target = batch\n",
    "        predicted = self.forward(text)\n",
    "        loss = self.loss(predicted, target)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_no):\n",
    "        prompt = batch[0]\n",
    "        tensorboard = self.logger.experiment\n",
    "        for temperature in [1, 0.5, 0.2, 0.1, 0.01]:\n",
    "            generated = self.generate(prompt, length=100, temperature=temperature)\n",
    "            tensorboard.add_text(f'val_generated_{temperature}_{batch_no}', generated, global_step=self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = TextTrainDataset(\n",
    "            self.hparams.text_file_path,\n",
    "            self.w2v,\n",
    "            self.hparams.seq_length,\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=24\n",
    "        )\n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        dataset = TextValidationDataset(\n",
    "            self.hparams.val_file_path,\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=1\n",
    "        )\n",
    "        \n",
    "    def __preprocess_prompt(self, prompt):\n",
    "        prompt = prompt.lower().strip()\n",
    "        prompt = re.sub(r'[^a-ząćęłńóśźż.,!? ]', '', prompt)\n",
    "        prompt = prompt.replace('.', ' . ').replace('!', ' ! ').replace('?', ' ? ').replace(',', ' , ')\n",
    "        prompt = prompt.split()\n",
    "        prompt = [word for word in prompt if word in self.w2v.wv]\n",
    "        padding = ['<pad>']*(max(self.hparams.seq_length-len(prompt), 0))\n",
    "        prompt = padding + prompt\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='../..',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    check_val_every_n_epoch=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LstmTextGenerator(\n",
    "    text_file_path='../../data/line_sentence/100k.txt',\n",
    "    val_file_path='../../data/line_sentence/texts_validation.txt',\n",
    "    word2vec_path='../../models/word2vec/100k/word2vec',\n",
    "    seq_length=25,\n",
    "    lstm_layers=3,\n",
    "    lstm_dropout=0.2,\n",
    "    lstm_hidden_size=100,\n",
    "    dropout=0.2,\n",
    "    bidirectional=True,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %\n",
       "============================================================================================================================================\n",
       "LstmTextGenerator                        [64, 20, 100]             [64, 36968]               --                             --\n",
       "├─LSTM: 1-1                              [64, 20, 100]             [64, 20, 200]             644,800                     7.98%\n",
       "├─Dropout: 1-2                           [64, 20, 200]             [64, 20, 200]             --                             --\n",
       "├─Linear: 1-3                            [64, 200]                 [64, 36968]               7,430,568                  92.02%\n",
       "============================================================================================================================================\n",
       "Total params: 8,075,368\n",
       "Trainable params: 8,075,368\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.30\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.51\n",
       "Forward/backward pass size (MB): 20.98\n",
       "Params size (MB): 32.30\n",
       "Estimated Total Size (MB): 53.79\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    generator,\n",
    "    input_size=(64, 20, 100),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | lstm    | LSTM             | 644 K \n",
      "1 | fc      | Linear           | 7.4 M \n",
      "2 | dropout | Dropout          | 0     \n",
      "3 | loss    | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "8.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.1 M     Total params\n",
      "32.301    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 109583it [25:50, 70.68it/s, v_num=22]\n",
      "Epoch 0: : 8891it [02:37, 56.48it/s, v_num=22]                             "
     ]
    }
   ],
   "source": [
    "trainer.fit(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma lasami jednego dłonie życie. stanie mi więc? chyba kiedy pozostać odparł, sir i się od wszystkich czasu tę całą łaskę krew. ślad jak wobec mnie cię dostał. leży od. gdzie nie pozostanie obecny przy stanie, lecz i pies panu nie o zachód, ale'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma lasami', temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dawno, dawno temu, za siedmioma górami i siedmioma lasami. a więc z tego będę, a nie jestem spotkamy. bądź zdrów, gdyż nie wiadomo, kim nie się. a więc nie wiadomo, kim nie powinienem, aby nie. nie mogę mi sobie, gdyż nie wiadomo o twoją. nie ma zamiaru mi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate('dawno, dawno temu, za siedmioma górami i siedmioma lasami', temperature=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
