{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from transformers import XLMTokenizer, AutoTokenizer\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformer import *\n",
    "from callback import GenerateCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.encode(\"witaj świecie\")\n",
    "# print(encoded)\n",
    "# decoded = tokenizer.decode(encoded)\n",
    "# print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import pad\n",
    "\n",
    "\n",
    "class TextTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, seq_length, padding=(3, 30), remove_dialogs=True, remove_special_chars=False, lowercase=False, tqdm=False, cache_path=None, cache_ignore=False, min_line_length=0):\n",
    "        self.samplesset_path = dataset_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        self.padding = padding\n",
    "        self.remove_dialogs = remove_dialogs\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "        self.lowercase = lowercase\n",
    "        self.tqdm = tqdm\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_ignore = cache_ignore\n",
    "        self.min_line_length = min_line_length\n",
    "        \n",
    "        self.samples = self.__get_samples()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.__add_random_padding(self.samples[idx])\n",
    "        src, tgt = sample[:-1], sample[1:]\n",
    "        return np.array(src, dtype=np.int64), np.array(tgt, dtype=np.int64)\n",
    "    \n",
    "    def __get_samples(self):\n",
    "        if self.cache_path is None or self.cache_ignore or not Path(self.cache_path).exists():\n",
    "            samples = self.__create_samples()\n",
    "            self.__save_samples_to_cache(samples)\n",
    "            return samples\n",
    "        else:\n",
    "            return self.__load_samples_from_cache()\n",
    "        \n",
    "    def __load_samples_from_cache(self):\n",
    "        with open(self.cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def __save_samples_to_cache(self, samples):\n",
    "        Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(self.cache_path, 'wb') as f:\n",
    "            return pickle.dump(samples, f)\n",
    "        \n",
    "    def __create_samples(self):\n",
    "        paths = list(glob.glob(f'{self.samplesset_path}/**/*.txt', recursive=True))\n",
    "        random.shuffle(paths)\n",
    "        data = []\n",
    "        \n",
    "        if self.tqdm:\n",
    "            paths = tqdm(paths)\n",
    "        \n",
    "        for path in paths:\n",
    "            text = self.__read_text_from_file(path)\n",
    "            samples = self.__get_samples_from_text(text)\n",
    "            data.extend(samples)\n",
    "                \n",
    "        return data\n",
    "                \n",
    "    def __get_samples_from_text(self, text):\n",
    "        samples = []\n",
    "        tokenized = self.tokenizer.encode(text)[1:-1]\n",
    "        \n",
    "        start_idx = -self.seq_length + self.padding[0]\n",
    "        end_idx = len(tokenized) - self.seq_length - 1\n",
    "        \n",
    "        for idx in range(start_idx, end_idx):\n",
    "            sequence = tokenized[max(idx, 0) : idx+self.seq_length+1]\n",
    "            samples.append(sequence)\n",
    "            \n",
    "        return samples\n",
    "\n",
    "    def __add_random_padding(self, sequence):\n",
    "        sequence_len = min(random.randint(self.padding[0], self.padding[1]), self.seq_length+1)\n",
    "        pad_sequence = pad(sequence[:sequence_len], self.seq_length+1, pad_token=self.tokenizer.pad_token_id)\n",
    "        return pad_sequence\n",
    "\n",
    "    def __read_text_from_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = map(self.__preprocess_line, lines)\n",
    "            lines = filter(lambda line: len(line) > self.min_line_length, lines)\n",
    "            if self.remove_dialogs:\n",
    "                lines = self.__remove_dialogs(lines)\n",
    "            text = '\\n'.join(lines)\n",
    "            if self.remove_special_chars:\n",
    "                text = re.sub(r'[^a-ząćęłńóśźż.,!? \\n]', ' ', text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "\n",
    "    def __remove_dialogs(self, lines):\n",
    "        return filter(lambda line: not self.__is_dialog_line(line), lines)\n",
    "    \n",
    "    def __preprocess_line(self, line):\n",
    "        line = line.strip()\n",
    "        if self.lowercase:\n",
    "            line = line.lower()\n",
    "        return line\n",
    "        \n",
    "    @staticmethod\n",
    "    def __is_dialog_line(line):\n",
    "        return '—' in line or '–' in line or '-' in line or '„' in line or '\"' in line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextTrainDataset(\n",
    "    '../../data/training',\n",
    "    tokenizer,\n",
    "    seq_length=SEQUENCE_LENGTH,\n",
    "    padding=(2, 50_000),\n",
    "    lowercase=False,\n",
    "    tqdm=True,\n",
    "    cache_path='.cache/dataset',\n",
    "    remove_dialogs=False,\n",
    "    remove_special_chars=False,\n",
    "    min_line_length=25,\n",
    "    # cache_ignore=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8934980\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in tqdm(dataset):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "a krew odpłynęła mu z twarzy. - Przyszło mi do głowy, że może być spokrewniona z Van Gouldem. - Nie daj Boże! - powiedział ledwie słyszalnym szeptem Czarny Korsarz. - To niemożliwe.Franciszek l <unk>Olonnais zatrzymał się pod gąszczem maot, wielkoliściastych drzew przypominających bawełniane krzewy, i uważnie przyjrzał się przyjacielowi. - Dlaczego tak na mnie patrzysz? - zapytał Czarny Korsarz. - Pomyślałem o twojej fla\n",
      "krew odpłynęła mu z twarzy. - Przyszło mi do głowy, że może być spokrewniona z Van Gouldem. - Nie daj Boże! - powiedział ledwie słyszalnym szeptem Czarny Korsarz. - To niemożliwe.Franciszek l <unk>Olonnais zatrzymał się pod gąszczem maot, wielkoliściastych drzew przypominających bawełniane krzewy, i uważnie przyjrzał się przyjacielowi. - Dlaczego tak na mnie patrzysz? - zapytał Czarny Korsarz. - Pomyślałem o twojej flaman\n"
     ]
    }
   ],
   "source": [
    "src, tgt = dataset[random.randint(0, len(dataset)-1)]\n",
    "print(len(src), len(tgt))\n",
    "print(tokenizer.decode(src))\n",
    "print(tokenizer.decode(tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cce = nn.CrossEntropyLoss()\n",
    "# pred = torch.randn((32, 50_000, 100))\n",
    "# target = torch.randint(0, 50_000-1, (32, 99))\n",
    "# loss = cce(pred, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "\n",
    "class TransformerLightning(LightningModule):\n",
    "    \n",
    "    def __init__(self, seq_length, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        \n",
    "        self.transformer = EncoderOnlyTransformer(\n",
    "            src_vocab_size=len(self.tokenizer),\n",
    "            d_model=512,\n",
    "            num_heads=8,\n",
    "            num_layers=6,\n",
    "            d_ff=2048,\n",
    "            max_seq_length=self.hparams.seq_length,\n",
    "            dropout=0.1,\n",
    "            mask_token=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id, label_smoothing=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "        \n",
    "    # def training_step(self, batch, batch_no):\n",
    "    #     src_data, tgt_data = batch\n",
    "    #     output = self(src_data)\n",
    "    #     predicted = output.contiguous().view(-1, len(self.tokenizer))\n",
    "    #     target = tgt_data.contiguous().view(-1)\n",
    "    #     loss = self.criterion(predicted, target.long())\n",
    "    #     self.log('train_loss', loss)\n",
    "    #     return loss\n",
    "    \n",
    "    # def training_step(self, batch, batch_no):\n",
    "    #     src_data, tgt_data = batch\n",
    "    #     output = self(src_data)\n",
    "    #     predicted = output[:, -1, :]\n",
    "    #     target = tgt_data[:, -1]\n",
    "    #     loss = self.criterion(predicted, target.long())\n",
    "    #     self.log('train_loss', loss)\n",
    "    #     return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_no):\n",
    "        src_data, tgt_data = batch\n",
    "        output = self(src_data).transpose(2, 1)\n",
    "        loss = self.criterion(output, tgt_data)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "        return optimizer\n",
    "    \n",
    "    def generate(self, prompt, length=50, temperature=0.5):\n",
    "        src_ids = self.tokenizer.encode(prompt)[1:-1]\n",
    "        generated_ids = self.__generate_ids(src_ids, length, temperature)\n",
    "        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def __generate_ids(self, src_ids, length=200, temperature=0.5):\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                input_ids = pad(src_ids[-self.hparams.seq_length:], self.hparams.seq_length, self.tokenizer.pad_token_id)\n",
    "                input_tensor = torch.unsqueeze(torch.tensor(input_ids, device=self.device), dim=0)\n",
    "                \n",
    "                output = self(input_tensor)\n",
    "                # tmp = output[0].argmax(axis=-1)\n",
    "                # print(self.tokenizer.decode(tmp))\n",
    "                word_idx = self.__sample_word_idx(output[0][-1], temperature)\n",
    "                src_ids.append(word_idx)\n",
    "            \n",
    "        self.train()\n",
    "        return src_ids\n",
    "        \n",
    "    @staticmethod\n",
    "    def __sample_word_idx(outputs, temperature=1.0):\n",
    "        scaled_logits = torch.log_softmax(outputs, dim=0) / temperature\n",
    "        adjusted_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "        return next_word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerLightning(seq_length=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Param %\n",
       "======================================================================================================================================================\n",
       "TransformerLightning                               [64, 100]                 [64, 100, 50560]          --                             --\n",
       "├─EncoderOnlyTransformer: 1-1                      [64, 100]                 [64, 100, 50560]          --                             --\n",
       "│    └─Embedding: 2-1                              [64, 100]                 [64, 100, 512]            25,886,720                 36.60%\n",
       "│    └─PositionalEncoding: 2-2                     [64, 100, 512]            [64, 100, 512]            --                             --\n",
       "│    └─Dropout: 2-3                                [64, 100, 512]            [64, 100, 512]            --                             --\n",
       "│    └─ModuleList: 2-4                             --                        --                        --                             --\n",
       "│    │    └─EncoderLayer: 3-1                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    │    └─EncoderLayer: 3-2                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    │    └─EncoderLayer: 3-3                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    │    └─EncoderLayer: 3-4                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    │    └─EncoderLayer: 3-5                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    │    └─EncoderLayer: 3-6                      [64, 100, 512]            [64, 100, 512]            3,152,384                   4.46%\n",
       "│    └─Linear: 2-5                                 [64, 100, 512]            [64, 100, 50560]          25,937,280                 36.67%\n",
       "======================================================================================================================================================\n",
       "Total params: 70,738,304\n",
       "Trainable params: 70,738,304\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.53\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 4345.04\n",
       "Params size (MB): 282.95\n",
       "Estimated Total Size (MB): 4628.04\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(\n",
    "    transformer,\n",
    "    input_size=(64, SEQUENCE_LENGTH),\n",
    "    col_names=['input_size', 'output_size', 'num_params', 'params_percent'],\n",
    "    dtypes=[torch.LongTensor],\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='.',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    every_n_train_steps=1000,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "generate_callback = GenerateCallback(\n",
    "    'Pewnego dnia czerwony kapturek szedł przez las z koszyczkiem jedzenia do swojej babci, która mieszkała w lesie. Śledził go jednak zły wilk, który chciał zjeść dziewczynkę. Dziewczynka szła wesoło przez las i niczego się nie spodziewała, kiedy',\n",
    "    temperatures=[0.01, 0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "    length=200,\n",
    "    interval=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    precision='16-mixed',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    log_every_n_steps=5,\n",
    "    callbacks=[generate_callback, checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.hparams.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                   | Params\n",
      "-------------------------------------------------------\n",
      "0 | transformer | EncoderOnlyTransformer | 70.7 M\n",
      "1 | criterion   | CrossEntropyLoss       | 0     \n",
      "-------------------------------------------------------\n",
      "70.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "70.7 M    Total params\n",
      "282.953   Total estimated model params size (MB)\n",
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▏         | 1256/89350 [09:03<10:35:31,  2.31it/s, v_num=62]"
     ]
    }
   ],
   "source": [
    "trainer.fit(transformer, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru,,,,,,,,,,,,,,,,,,,,,,,,,.,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru', temperature=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
