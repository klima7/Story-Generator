{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klima7/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from transformers import XLMTokenizer, AutoTokenizer\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformer import *\n",
    "from callback import GenerateCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50560"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.encode(\"witaj świecie\")\n",
    "# print(encoded)\n",
    "# decoded = tokenizer.decode(encoded)\n",
    "# print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import pad\n",
    "\n",
    "\n",
    "class TextTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, min_src_length, max_src_length, tgt_length, remove_dialogs=True, remove_special_chars=False, lowercase=False, tqdm=False, cache_path=None, cache_ignore=False, min_line_length=0):\n",
    "        self.samplesset_path = dataset_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_src_length = min_src_length\n",
    "        self.max_src_length = max_src_length\n",
    "        self.tgt_length = tgt_length\n",
    "        self.remove_dialogs = remove_dialogs\n",
    "        self.remove_special_chars = remove_special_chars\n",
    "        self.lowercase = lowercase\n",
    "        self.tqdm = tqdm\n",
    "        self.cache_path = cache_path\n",
    "        self.cache_ignore = cache_ignore\n",
    "        self.min_line_length = min_line_length\n",
    "        \n",
    "        self.samples = self.__get_samples()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx]\n",
    "        src, tgt = self.__split_into_src_tgt(text)\n",
    "        return np.array(src, dtype=np.int32), np.array(tgt, dtype=np.int32)\n",
    "    \n",
    "    def __get_samples(self):\n",
    "        if self.cache_path is None or self.cache_ignore or not Path(self.cache_path).exists():\n",
    "            samples = self.__create_samples()\n",
    "            self.__save_samples_to_cache(samples)\n",
    "            return samples\n",
    "        else:\n",
    "            return self.__load_samples_from_cache()\n",
    "        \n",
    "    def __load_samples_from_cache(self):\n",
    "        with open(self.cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n",
    "    def __save_samples_to_cache(self, samples):\n",
    "        Path(self.cache_path).parent.mkdir(parents=True, exist_ok=True) \n",
    "        with open(self.cache_path, 'wb') as f:\n",
    "            return pickle.dump(samples, f)\n",
    "        \n",
    "    def __create_samples(self):\n",
    "        paths = list(glob.glob(f'{self.samplesset_path}/**/*.txt', recursive=True))\n",
    "        random.shuffle(paths)\n",
    "        data = []\n",
    "        \n",
    "        if self.tqdm:\n",
    "            paths = tqdm(paths)\n",
    "        \n",
    "        for path in paths:\n",
    "            text = self.__read_text_from_file(path)\n",
    "            samples = self.__get_samples_from_text(text)\n",
    "            data.extend(samples)\n",
    "                \n",
    "        return data\n",
    "                \n",
    "    def __get_samples_from_text(self, text):\n",
    "        samples = []\n",
    "        tokenized = self.tokenizer.encode(text)\n",
    "        \n",
    "        min_length = self.min_src_length + 1\n",
    "        \n",
    "        start_idx = 0\n",
    "        end_idx = len(tokenized) - min_length - 1\n",
    "        \n",
    "        for idx in range(start_idx, end_idx):\n",
    "            sequence = tokenized[idx : idx+self.max_src_length+self.tgt_length]\n",
    "            samples.append(sequence)\n",
    "            \n",
    "        return samples\n",
    "    \n",
    "    def __split_into_src_tgt(self, text):\n",
    "        max_src_length = min(len(text)-1, self.max_src_length)\n",
    "        src_length = random.randint(self.min_src_length, max_src_length) if self.min_src_length != max_src_length else max_src_length\n",
    "        \n",
    "        src = text[:src_length]\n",
    "        tgt = text[src_length:src_length+self.tgt_length-1]\n",
    "        tgt.insert(0, self.tokenizer.bos_token_id)\n",
    "        \n",
    "        src = pad(src, self.tgt_length, pad_token=self.tokenizer.pad_token_id)\n",
    "        tgt = pad(tgt, self.tgt_length, pad_token=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        return src, tgt\n",
    "\n",
    "    def __read_text_from_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = map(self.__preprocess_line, lines)\n",
    "            lines = filter(lambda line: len(line) > self.min_line_length, lines)\n",
    "            if self.remove_dialogs:\n",
    "                lines = self.__remove_dialogs(lines)\n",
    "            text = '\\n'.join(lines)\n",
    "            if self.remove_special_chars:\n",
    "                text = re.sub(r'[^a-ząćęłńóśźż.,!? \\n]', ' ', text, flags=re.IGNORECASE)\n",
    "            return text\n",
    "\n",
    "    def __remove_dialogs(self, lines):\n",
    "        return filter(lambda line: not self.__is_dialog_line(line), lines)\n",
    "    \n",
    "    def __preprocess_line(self, line):\n",
    "        line = line.strip()\n",
    "        if self.lowercase:\n",
    "            line = line.lower()\n",
    "        return line\n",
    "        \n",
    "    @staticmethod\n",
    "    def __is_dialog_line(line):\n",
    "        return '—' in line or '–' in line or '-' in line or '„' in line or '\"' in line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextTrainDataset(\n",
    "    '../../data/training_trans',\n",
    "    tokenizer,\n",
    "    min_src_length=6,\n",
    "    max_src_length=20,\n",
    "    tgt_length=100,\n",
    "    lowercase=True,\n",
    "    tqdm=True,\n",
    "    cache_path='.cache/dataset',\n",
    "    # cache_ignore=True,\n",
    "    remove_special_chars=True,\n",
    "    min_line_length=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/174852 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (6, 3, -3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(dataset):\n\u001b[1;32m      2\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mTextTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     36\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[idx]\n\u001b[0;32m---> 37\u001b[0m     src, tgt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__split_into_src_tgt(text)\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(src, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32), np\u001b[39m.\u001b[39marray(tgt, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)\n",
      "Cell \u001b[0;32mIn[7], line 89\u001b[0m, in \u001b[0;36mTextTrainDataset.__split_into_src_tgt\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__split_into_src_tgt\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m     88\u001b[0m     max_src_length \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(text)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_src_length)\n\u001b[0;32m---> 89\u001b[0m     src_length \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mrandint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_src_length, max_src_length) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_src_length \u001b[39m!=\u001b[39m max_src_length \u001b[39melse\u001b[39;00m max_src_length\n\u001b[1;32m     91\u001b[0m     src \u001b[39m=\u001b[39m text[:src_length]\n\u001b[1;32m     92\u001b[0m     tgt \u001b[39m=\u001b[39m text[src_length:src_length\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_length\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/random.py:370\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandint\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[1;32m    367\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandrange(a, b\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/studies/piat/Story-Generator/conda/pytorch/lib/python3.10/random.py:353\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m width \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    352\u001b[0m         \u001b[39mreturn\u001b[39;00m istart \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(width)\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mempty range for randrange() (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (istart, istop, width))\n\u001b[1;32m    355\u001b[0m \u001b[39m# Non-unit step argument supplied.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m istep \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (6, 3, -3)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for x in tqdm(dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "[    2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "     2     2     2     2     2     2     2     2     2     2 27873  1074\n",
      "   990  2313   281 20728  1336    14  2313    14  2480    20    22  8782\n",
      "  1157    15   256   824]\n",
      "[    0   756  6652    18    37   407 48181  1846    15  3027  1063 18547\n",
      "    20    14    26 23446  4590 22231  2262    91 14295  2382    15    25\n",
      "  2893  1423  1234 44052   955    15   601    39  2277    14  2442    20\n",
      "  8095   146    19   254    16   358 10202  2105  6154 15136  1619   429\n",
      "    20    14    25 11334   260  2382   990    32   470  5849 17705   937\n",
      "   281 20728  1336    14   722   529    24    16  3410  5902    15    49\n",
      " 15375    14    94    41    45    42  2442   137  8311   441   756    41\n",
      "  2998 48580    15  5874  2888    43    14   170    18   605 10073    14\n",
      "    30   281 20728  1336]\n"
     ]
    }
   ],
   "source": [
    "src, tgt = dataset[random.randint(0, len(dataset)-1)]\n",
    "print(len(src), len(tgt))\n",
    "print(src)\n",
    "print(tgt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from lightning.pytorch import LightningModule\n",
    "\n",
    "\n",
    "class TransformerLightning(LightningModule):\n",
    "    \n",
    "    def __init__(self, seq_length, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            src_vocab_size=len(self.tokenizer),\n",
    "            tgt_vocab_size=len(self.tokenizer),\n",
    "            d_model=512,\n",
    "            num_heads=8,\n",
    "            num_layers=6,\n",
    "            d_ff=2048,\n",
    "            max_seq_length=100,\n",
    "            dropout=0.1,\n",
    "            mask_token=self.tokenizer.pad_token_id,\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        return self.transformer(src, tgt)\n",
    "        \n",
    "    def training_step(self, batch, batch_no):\n",
    "        src_data, tgt_data = batch\n",
    "        output = self(src_data, tgt_data[:, :-1])\n",
    "        predicted = output.contiguous().view(-1, len(self.tokenizer))\n",
    "        target = tgt_data[:, 1:].contiguous().view(-1)\n",
    "        loss = self.criterion(predicted, target.long())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "        return optimizer\n",
    "    \n",
    "    def generate(self, prompt, length=50, temperature=0.5):\n",
    "        src_ids = self.tokenizer.encode(prompt)[1:-1]\n",
    "        generated_ids = self.__generate_ids(src_ids, length, temperature)\n",
    "        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def __generate_ids(self, src_ids, length=200, temperature=0.5):\n",
    "        src_ids = pad(src_ids, self.hparams.seq_length, self.tokenizer.pad_token_id)\n",
    "        tgt_ids = [self.tokenizer.bos_token_id]\n",
    "        \n",
    "        src_tensor = torch.unsqueeze(torch.tensor(src_ids, device=self.device), dim=0)\n",
    "        \n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(length):\n",
    "                tgt_padded = pad(tgt_ids, self.hparams.seq_length, self.tokenizer.pad_token_id)\n",
    "                tgt_tensor = torch.unsqueeze(torch.tensor(tgt_padded, device=self.device), dim=0)\n",
    "                \n",
    "                # pos = self.hparams.seq_length - len(tgt_ids)\n",
    "                output = self(src_tensor, tgt_tensor).squeeze(0)[-1]\n",
    "                word_idx = self.__sample_word_idx(output, temperature)\n",
    "                tgt_ids.append(word_idx)\n",
    "            \n",
    "        self.train()\n",
    "        return src_ids + tgt_ids[1:]\n",
    "        \n",
    "    @staticmethod\n",
    "    def __sample_word_idx(outputs, temperature=1.0):\n",
    "        scaled_logits = torch.log_softmax(outputs, dim=0) / temperature\n",
    "        adjusted_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(adjusted_probs, num_samples=1).item()\n",
    "        return next_word_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerLightning(seq_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = TensorBoardLogger(\n",
    "    save_dir='.',\n",
    "    name='logs'\n",
    ")\n",
    "\n",
    "generate_callback = GenerateCallback(\n",
    "    'Pewnego dnia czerwony kapturek szedł przez las z koszyczkiem jedzenia do swojej babci, która mieszkała w lesie. Śledził go jednak zły wilk, który chciał zjeść dziewczynkę.',\n",
    "    temperatures=[0.01, 0.1, 0.2, 0.3, 0.5, 0.7],\n",
    "    length=100,\n",
    "    interval=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='cuda',\n",
    "    precision='16-mixed',\n",
    "    max_epochs=-1,\n",
    "    enable_progress_bar=True,\n",
    "    logger = logger,\n",
    "    callbacks=[generate_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(transformer, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n",
      "torch.Size([50560])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer\u001b[39m.\u001b[39;49mgenerate(\u001b[39m'\u001b[39;49m\u001b[39mPewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru\u001b[39;49m\u001b[39m'\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[15], line 46\u001b[0m, in \u001b[0;36mTransformerLightning.generate\u001b[0;34m(self, prompt, length, temperature)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, prompt, length\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n\u001b[1;32m     45\u001b[0m     src_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(prompt)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 46\u001b[0m     generated_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__generate_ids(src_ids, length, temperature)\n\u001b[1;32m     47\u001b[0m     generated_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m generated_text\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36mTransformerLightning.__generate_ids\u001b[0;34m(self, src_ids, length, temperature)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[39m# pos = self.hparams.seq_length - len(tgt_ids)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(src_tensor, tgt_tensor)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m         word_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__sample_word_idx(output, temperature)\n\u001b[1;32m     66\u001b[0m         tgt_ids\u001b[39m.\u001b[39mappend(word_idx)\n\u001b[1;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[15], line 71\u001b[0m, in \u001b[0;36mTransformerLightning.__sample_word_idx\u001b[0;34m(outputs, temperature)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m src_ids \u001b[39m+\u001b[39m tgt_ids[\u001b[39m1\u001b[39m:]\n\u001b[0;32m---> 71\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sample_word_idx\u001b[39m(outputs, temperature\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     74\u001b[0m     scaled_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog_softmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m temperature\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.generate('Pewnego słonecznego dnia czerwony kapturek szedł do swojej babci z koszyczkiem. Kapturek był koloru', temperature=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([64, 99, 5000])\n",
      "torch.Size([6336, 5000]) torch.Size([6336])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    print('output', output.shape)\n",
    "    predicted = output.contiguous().view(-1, tgt_vocab_size)\n",
    "    target = tgt_data[:, 1:].contiguous().view(-1)\n",
    "    print(predicted.shape, target.shape)\n",
    "    break\n",
    "    loss = criterion(predicted, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
